{"idx": 0, "code": "def XXXXX(CNN=None, second=10, saveable=True, name='cnn', fig_idx=3119362):\n    import matplotlib.pyplot as plt\n    \n    # tl.logging.info(CNN.shape)    # (5, 5, 3, 64) # exit()\n    n_mask = CNN.shape[3]\n    n_row = CNN.shape[0]\n    n_col = CNN.shape[1]\n    n_color = CNN.shape[2]\n    \n    row = int(np.sqrt(n_mask))\n    col = int(np.ceil(n_mask / row))\n    \n    plt.ion()  # active mode\n    fig = plt.figure(fig_idx)\n    count = 1\n    \n    for _ir in range(1, row + 1):\n        for _ic in range(1, col + 1):\n            if count > n_mask:\n                break\n            fig.add_subplot(col, row, count)\n            \n            # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col)   # (5, 1, 32) 5 5 # exit()\n            # plt.imshow(\n            #         np.reshape(CNN[count-1,:,:,:], (n_row, n_col)),\n            #         cmap='gray', interpolation=\"nearest\")     # theano\n            \n            if n_color == 1:\n                plt.imshow(np.reshape(CNN[:, :, :, count - 1], (n_row, n_col)),\n                           cmap='gray', interpolation=\"nearest\")\n            elif n_color == 3:\n                plt.imshow(np.reshape(CNN[:, :, :, count - 1], (n_row, n_col, n_color)),\n                           cmap='gray', interpolation=\"nearest\")\n            else:\n                raise Exception(\"Unknown n_color\")\n            \n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n            \n            count = count + 1\n    \n    if saveable:\n        plt.savefig(name + '.pdf', format='pdf')\n    else:\n        plt.draw()\n        plt.pause(second)", "Function Name 1": "CNN2d", "Function Name 2": "plotcn", "Function Name 3": "plotcnn", "Function Name 4": "visualize_cnn"}
{"idx": 1, "code": "@llrule(loc, lambda parser: [kind])\ndef XXXXX(kind, loc=None):\n    def rule(parser):\n        result = parser._accept(kind)\n        if result is unmatched:\n            return result\n        return result.loc\n    \n    return rule", "Function Name 1": "Loc", "Function Name 2": "lazy", "Function Name 3": "acceptrule", "Function Name 4": "rule_definition"}
{"idx": 2, "code": "def XXXXX(self, board, layers, things, the_plot):\n    # Disappear if we've hit a bunker.\n    if self.character in the_plot['bunker_hitters']:\n        return self._teleport((-1, -1))\n    \n    # End the game if we've hit the player.\n    if self.position == things['P'].position:\n        the_plot.terminate_episode()\n        self._south(board, the_plot)", "Function Name 1": "_fly", "Function Name 2": "pellfl", "Function Name 3": "onhit", "Function Name 4": "handle_collision"}
{"idx": 3, "code": "def XXXXX(cpu, dest, src):\n    # XXX bypass a capstone bug that incorrectly extends and computes operands sizes\n    # the bug has been fixed since capstone 4.0.alpha2 (commit de8dd26)\n    if src.size == 64 and src.type == 'immediate' and dest.size == 64:\n        arg1 = Operators.SEXTEND(src.read(), 32, 64)\n    else:\n        arg1 = src.read()\n\n    res = dest.write(dest.read() & arg1)\n\n    # Defined Flags: szp\n    cpu._calculate_logic_flags(dest.size, res)", "Function Name 1": "AND", "Function Name 2": "flagsand", "Function Name 3": "copy", "Function Name 4": "bypasscapstonebug"}
{"idx": 4, "code": "def XXXXX(cpu, src):\n    reg_name_h = {8: 'AH', 16: 'DX', 32: 'EDX', 64: 'RDX'}[src.size]\n    reg_name_l = {8: 'AL', 16: 'AX', 32: 'EAX', 64: 'RAX'}[src.size]\n\n    dividend = Operators.CONCAT(src.size * 2, cpu.read_register(reg_name_h), cpu.read_register(reg_name_l))\n    divisor = src.read()\n\n    if isinstance(divisor, int) and divisor == 0:\n        raise DivideByZeroError()\n\n    dst_size = src.size * 2\n    divisor = Operators.SEXTEND(divisor, src.size, dst_size)\n\n    mask = (1 << dst_size) - 1\n    sign_mask = 1 << (dst_size - 1)\n\n    dividend_sign = (dividend & sign_mask) != 0\n    divisor_sign = (divisor & sign_mask) != 0\n\n    if isinstance(divisor, int):\n        if divisor_sign:\n            divisor = ((~divisor) + 1) & mask\n            divisor = -divisor\n\n    if isinstance(dividend, int):\n        if dividend_sign:\n            dividend = ((~dividend) + 1) & mask\n            dividend = -dividend\n\n    quotient = Operators.SDIV(dividend, divisor)\n\n    if isinstance(dividend, int) and isinstance(dividend, int):\n        # handle the concrete case\n        remainder = dividend - (quotient * divisor)\n    else:\n        # symbolic case -- optimize via SREM\n        remainder = Operators.SREM(dividend, divisor)\n\n    cpu.write_register(reg_name_l, Operators.EXTRACT(quotient, 0, src.size))\n    cpu.write_register(reg_name_h, Operators.EXTRACT(remainder, 0, src.size))", "Function Name 1": "IDIV", "Function Name 2": "opslogicalm", "Function Name 3": "divideby", "Function Name 4": "divide"}
{"idx": 5, "code": "def XXXXX(cpu, dest):\n    dest.write(Operators.ITEBV(dest.size, cpu.ZF, 1, 0))", "Function Name 1": "SETZ", "Function Name 2": "opsbranchesm", "Function Name 3": "writezero", "Function Name 4": "conditionalwrite"}
{"idx": 6, "code": "def XXXXX(cpu, op0):\n    # TODO FIX 64Bit FIX segment\n    proc = op0.read()\n    cpu.push(cpu.PC, cpu.address_bit_size)\n    cpu.PC = proc", "Function Name 1": "CALL", "Function Name 2": "processorcall", "Function Name 3": "cpubaseread", "Function Name 4": "load_and_execute"}
{"idx": 7, "code": "def XXXXX(self, recipient):\n    # This may create a user account\n    recipient = Operators.EXTRACT(recipient, 0, 160)\n    address = self.address\n    \n    # FIXME for on the known addresses\n    if issymbolic(recipient):\n        logger.info(\"Symbolic recipient on self destruct\")\n        recipient = solver.get_value(self.constraints, recipient)\n    \n    if recipient not in self.world:\n        self.world.create_account(address=recipient)\n        self.world.send_funds(address, recipient, self.world.get_balance(address))\n        self.world.delete_account(address)\n    \n    raise EndTx('SELFDESTRUCT')", "Function Name 1": "SELFDESTRUCT", "Function Name 2": "election", "Function Name 3": "destruct", "Function Name 4": "createandtransferfunds"}
{"idx": 8, "code": "def XXXXX(self):\n    logger.info(\"DFK cleanup initiated\")\n\n    # this check won't detect two DFK cleanups happening from\n    # different threads extremely close in time because of\n    # non-atomic read/modify of self.cleanup_called\n    if self.cleanup_called:\n        raise Exception(\"attempt to clean up DFK when it has already been cleaned-up\")\n\n    self.cleanup_called = True\n    self.log_task_states()\n\n    # Checkpointing takes priority over the rest of the tasks\n    # checkpoint if any valid checkpoint method is specified\n    if self.checkpoint_mode is not None:\n        self.checkpoint()\n\n    if self._checkpoint_timer:\n        logger.info(\"Stopping checkpoint timer\")\n        self._checkpoint_timer.close()\n\n    # Send final stats\n    self.usage_tracker.send_message()\n    self.usage_tracker.close()\n\n    logger.info(\"Terminating flow_control and strategy threads\")\n    self.flowcontrol.close()\n\n    for executor in self.executors.values():\n        if executor.managed:\n            if executor.scaling_enabled:\n                job_ids = executor.provider.resources.keys()\n                executor.scale_in(len(job_ids))\n            executor.shutdown()\n\n    self.time_completed = datetime.datetime.now()\n\n    if self.monitoring:\n        self.monitoring.send(MessageType.WORKFLOW_INFO, {\n            'tasks_failed_count': self.tasks_failed_count,\n            'tasks_completed_count': self.tasks_completed_count,\n            \"time_began\": self.time_began,\n            'time_completed': self.time_completed,\n            'workflow_duration': (self.time_completed - self.time_began).total_seconds(),\n            'run_id': self.run_id,\n            'rundir': self.run_dir\n        })\n\n        self.monitoring.close()\n\n    \"\"\"\n    if self.logging_server is not None:\n        self.logging_server.terminate()\n        self.logging_server.join()\n\n    if self.web_app is not None:\n        self.web_app.terminate()\n        self.web_app.join()\n    \"\"\"\n\n    logger.info(\"DFK cleanup complete\")", "Function Name 1": "cleanup", "Function Name 2": "dfaserver", "Function Name 3": "initdfk", "Function Name 4": "dfk_initiated"}
{"idx": 9, "code": "def XXXXX(self, my_task):\n    assert my_task is not None\n    self.test()\n\n    # Acquire locks, if any.\n    for lock in self.locks:\n        mutex = my_task.workflow._get_mutex(lock)\n        if not mutex.testandset():\n            return\n\n    # Assign variables, if so requested.\n    for assignment in self.pre_assign:\n        assignment.assign(my_task, my_task)\n\n    # Run task-specific code.\n    self._on_ready_before_hook(my_task)\n    self.reached_event.emit(my_task.workflow, my_task)\n    self._on_ready_hook(my_task)\n\n    # Run user code, if any.\n    if self.ready_event.emit(my_task.workflow, my_task):\n\n        # Assign variables, if so requested.\n        for assignment in self.post_assign:\n            assignment.assign(my_task, my_task)\n\n        # Release locks, if any.\n        for lock in self.locks:\n            mutex = my_task.workflow._get_mutex(lock)\n            mutex.unlock()\n\n        self.finished_event.emit(my_task.workflow, my_task)", "Function Name 1": "_on_ready", "Function Name 2": "taskon", "Function Name 3": "reach", "Function Name 4": "execute_task"}
{"idx": 10, "code": "def XXXXX(model, reaction_list=None, loopless=False, fraction_of_optimum=1.0, pfba_factor=None, processes=None):\n    if reaction_list is None:\n        reaction_ids = [r.id for r in model.reactions]\n    else:\n        reaction_ids = [r.id for r in model.reactions.get_by_any(reaction_list)]\n\n    if processes is None:\n        processes = CONFIGURATION.processes\n\n    num_reactions = len(reaction_ids)\n    processes = min(processes, num_reactions)\n\n    fva_result = DataFrame({\"minimum\": zeros(num_reactions, dtype=float), \"maximum\": zeros(num_reactions, dtype=float)}, index=reaction_ids)\n    prob = model.problem\n\n    with model:\n        # Safety check before setting up FVA.\n        model.slim_optimize(error_value=None, message=\"There is no optimal solution for the chosen objective!\")\n\n        # Add the previous objective as a variable to the model then set it to zero.\n        # This also uses the fraction to create the lower/upper bound for the old objective.\n        # TODO: Use utility function here (fix_objective_as_constraint)?\n        if model.solver.objective.direction == \"max\":\n            fva_old_objective = prob.Variable(\"fva_old_objective\", lb=fraction_of_optimum * model.solver.objective.value)\n        else:\n            fva_old_objective = prob.Variable(\"fva_old_objective\", ub=fraction_of_optimum * model.solver.objective.value)\n\n        fva_old_obj_constraint = prob.Constraint(model.solver.objective.expression - fva_old_objective, lb=0, ub=0, name=\"fva_old_objective_constraint\")\n        model.add_cons_vars([fva_old_objective, fva_old_obj_constraint])\n\n        if pfba_factor is not None:\n            if pfba_factor < 1.:\n                warn(\"The 'pfba_factor' should be larger or equal to 1.\", UserWarning)\n\n            with model:\n                add_pfba(model, fraction_of_optimum=0)\n                ub = model.slim_optimize(error_value=None)\n\n                flux_sum = prob.Variable(\"flux_sum\", ub=pfba_factor * ub)\n                flux_sum_constraint = prob.Constraint(model.solver.objective.expression - flux_sum, lb=0, ub=0, name=\"flux_sum_constraint\")\n                model.add_cons_vars([flux_sum, flux_sum_constraint])\n\n            model.objective = Zero  # This will trigger the reset as well\n\n        for what in (\"minimum\", \"maximum\"):\n            if processes > 1:\n                # We create and destroy a new pool here in order to set the\n                # objective direction for all reactions. This creates a\n                # slight overhead but seems the most clean.\n                chunk_size = len(reaction_ids) // processes\n                pool = multiprocessing.Pool(processes, initializer=_init_worker, initargs=(model, loopless, what[:3]))\n\n                for rxn_id, value in pool.imap_unordered(_fva_step, reaction_ids, chunksize=chunk_size):\n                    fva_result.at[rxn_id, what] = value\n\n                pool.close()\n                pool.join()\n            else:\n                _init_worker(model, loopless, what[:3])\n\n                for rxn_id, value in map(_fva_step, reaction_ids):\n                    fva_result.at[rxn_id, what] = value\n\n    return fva_result[[\"minimum\", \"maximum\"]]", "Function Name 1": "flux_variability_analysis", "Function Name 2": "calculate", "Function Name 3": "getfva", "Function Name 4": "performfva"}
{"idx": 11, "code": "def XXXXX(CASRN, AvailableMethods=False, Method=None):\n    def list_methods():\n        methods = []\n        if CASRN in GWP_data.index:\n            methods.append(IPCC100)\n            if not pd.isnull(GWP_data.at[CASRN, 'SAR 100yr']):\n                methods.append(IPCC100SAR)\n            methods.append(IPCC20)\n            methods.append(IPCC500)\n            methods.append(NONE)\n        return methods\n\n    if AvailableMethods:\n        return list_methods()\n\n    if not Method:\n        Method = list_methods()[0]\n\n    if Method == IPCC100:\n        return float(GWP_data.at[CASRN, '100yr GWP'])\n    elif Method == IPCC100SAR:\n        return float(GWP_data.at[CASRN, 'SAR 100yr'])\n    elif Method == IPCC20:\n        return float(GWP_data.at[CASRN, '20yr GWP'])\n    elif Method == IPCC500:\n        return float(GWP_data.at[CASRN, '500yr GWP'])\n    elif Method == NONE:\n        return None\n    else:\n        raise Exception('Failure in function')", "Function Name 1": "GWP", "Function Name 2": "cip", "Function Name 3": "gaussian", "Function Name 4": "get_flood_risk_value"}
{"idx": 12, "code": "def XXXXX(self, location=1, normalize=True, activity_threshold=7.0, min_activity_duration=0.25,\n        initial_search_buffer=1.0, max_gap=0.25, initial_pad=0.0):\n    if location not in [-1, 1]:\n        raise ValueError(\"location must be -1 or 1.\")\n    if not isinstance(normalize, bool):\n        raise ValueError(\"normalize must be a boolean.\")\n    if not is_number(activity_threshold):\n        raise ValueError(\"activity_threshold must be a number.\")\n    if not is_number(min_activity_duration) or min_activity_duration < 0:\n        raise ValueError(\"min_activity_duration must be a positive number\")\n    if not is_number(initial_search_buffer) or initial_search_buffer < 0:\n        raise ValueError(\"initial_search_buffer must be a positive number\")\n    if not is_number(max_gap) or max_gap < 0:\n        raise ValueError(\"max_gap must be a positive number.\")\n    if not is_number(initial_pad) or initial_pad < 0:\n        raise ValueError(\"initial_pad must be a positive number.\")\n\n    effect_args = []\n    if normalize:\n        effect_args.append('norm')\n    if location == -1:\n        effect_args.append('reverse')\n\n    effect_args.extend(['vad', '-t', '{:f}'.format(activity_threshold),\n                        '-T', '{:f}'.format(min_activity_duration),\n                        '-s', '{:f}'.format(initial_search_buffer),\n                        '-g', '{:f}'.format(max_gap),\n                        '-p', '{:f}'.format(initial_pad)])\n\n    if location == -1:\n        effect_args.append('reverse')\n\n    self.effects.extend(effect_args)\n    self.effects_log.append('vad')\n    return self", "Function Name 1": "vad", "Function Name 2": "basetr", "Function Name 3": "createsearchqueue", "Function Name 4": "configure_effects"}
{"idx": 13, "code": "def XXXXX(ctx, pager, limit, twtfile, sorting, timeout, porcelain, source, cache, force_update):\n    if source:\n        source_obj = ctx.obj[\"conf\"].get_source_by_nick(source)\n        if not source_obj:\n            logger.debug(\"Not following {0}, trying as URL\".format(source))\n            source_obj = Source(source, source)\n        sources = [source_obj]\n    else:\n        sources = ctx.obj[\"conf\"].following\n\n    tweets = []\n    if cache:\n        try:\n            with Cache.discover(update_interval=ctx.obj[\"conf\"].timeline_update_interval) as cache:\n                force_update = force_update or not cache.is_valid\n                if force_update:\n                    tweets = get_remote_tweets(sources, limit, timeout, cache)\n                else:\n                    logger.debug(\"Multiple calls to 'timeline' within {0} seconds. Skipping update\".format(cache.update_interval))\n                    # Behold, almighty list comprehensions! (I might have gone overboard here…)\n                    tweets = list(chain.from_iterable([cache.get_tweets(source.url) for source in sources]))\n        except OSError as e:\n            logger.debug(e)\n            tweets = get_remote_tweets(sources, limit, timeout)\n    else:\n        tweets = get_remote_tweets(sources, limit, timeout)\n\n    if twtfile and not source:\n        source = Source(ctx.obj[\"conf\"].nick, ctx.obj[\"conf\"].twturl, file=twtfile)\n        tweets.extend(get_local_tweets(source, limit))\n\n    if not tweets:\n        return\n\n    tweets = sort_and_truncate_tweets(tweets, sorting, limit)\n\n    if pager:\n        click.echo_via_pager(style_timeline(tweets, porcelain))\n    else:\n        click.echo(style_timeline(tweets, porcelain))", "Function Name 1": "timeline", "Function Name 2": "follow", "Function Name 3": "followers", "Function Name 4": "fetch_tweets"}
{"idx": 14, "code": "def XXXXX(github_user, template, long_format):\n    github_urls = temple.ls.ls(github_user, template=template)\n\n    for ssh_path, info in github_urls.items():\n        if long_format:\n            print(ssh_path, '-', info['description'] or '(no project description found)')\n        else:\n            print(ssh_path)", "Function Name 1": "ls", "Function Name 2": "download", "Function Name 3": "getgithubprojects", "Function Name 4": "print_github_repo_info"}
{"idx": 15, "code": "def XXXXX(ctx, no_publish=False, clean=False, opts=''):\n    cfg = config.load()\n\n    if clean:\n        ctx.run(\"invoke clean --docs\")\n\n    cmd = ['sphinx-build', '-b', 'confluence']\n    cmd.extend(['-E', '-a'])\n\n    # force a full rebuild\n    if opts:\n        cmd.append(opts)\n\n    cmd.extend(['.', ctx.rituals.docs.build + '_cf'])\n\n    if no_publish:\n        cmd.extend(['-Dconfluence_publish=False'])\n\n    # Build docs\n    notify.info(\"Starting Sphinx build...\")\n\n    with pushd(ctx.rituals.docs.sources):\n        ctx.run(' '.join(cmd), pty=True)", "Function Name 1": "confluence", "Function Name 2": "build", "Function Name 3": "builddocs", "Function Name 4": "build_docs"}
{"idx": 16, "code": "def XXXXX(self, T):\n    result = 0.0\n\n    if T < self.Tmax:\n        lT = T\n    else:\n        lT = self.Tmax\n\n    Tref = self.Tmin\n\n    for c, e in zip(self._coefficients, self._exponents):\n        # Analytically integrate Cp(T).\n        if e == -1.0:\n            result += c * math.log(lT / Tref)\n        else:\n            result += c * (lT ** (e + 1.0) - Tref ** (e + 1.0)) / (e + 1.0)\n\n    return result", "Function Name 1": "H", "Function Name 2": "orbitinfos", "Function Name 3": "gauss", "Function Name 4": "calculate_integration_result"}
{"idx": 17, "code": "def XXXXX(port, RTS, DTR):\n    port.setRTS(RTS)\n    port.setDTR(DTR)", "Function Name 1": "_setRTSDTR", "Function Name 2": "setport", "Function Name 3": "setportproperties", "Function Name 4": "setportsettings"}
{"idx": 18, "code": "def XXXXX(self, time: int = None) -> bool:\n    if time is None:\n        epoch = datetime(1970, 1, 1, 0, 0, 0)\n        now = datetime.utcnow()\n        time = int((now - epoch).total_seconds())\n\n    if isinstance(self.valid_from, int) and time < self.valid_from:\n        return False\n\n    if isinstance(self.valid_to, int) and time > self.valid_to:\n        return False\n\n    return True", "Function Name 1": "valid", "Function Name 2": "accesst", "Function Name 3": "isoutof", "Function Name 4": "is_within_time_range"}
{"idx": 19, "code": "def XXXXX(ip):\n    bits = []\n\n    while ip:\n        bits.append(_BYTES_TO_BITS[ip & 255])\n        ip >>= 8\n\n    bits.reverse()\n    return ''.join(bits) or 32 * '0'", "Function Name 1": "_dec_to_bin", "Function Name 2": "ipv4", "Function Name 3": "iptohex", "Function Name 4": "iptobinarystring"}
{"idx": 20, "code": "def XXXXX(fname=False, save=True, show=None):\n    if fname and os.path.exists(fname.replace(\".abf\", \".rst\")):\n        print(\"SKIPPING DUE TO RST FILE\")\n        return\n\n    swhlab.plotting.core.IMAGE_SAVE = save\n\n    if show is None:\n        if cm.isIpython():\n            swhlab.plotting.core.IMAGE_SHOW = True\n        else:\n            swhlab.plotting.core.IMAGE_SHOW = False\n\n    abf = ABF(fname)  # ensure it's a class\n    print(\">>>>> PROTOCOL >>>>>\", abf.protocomment)\n\n    runFunction = \"proto_unknown\"\n\n    if \"proto_\" + abf.protocomment in globals():\n        runFunction = \"proto_\" + abf.protocomment\n\n    abf.log.debug(\"running %s()\" % (runFunction))\n    plt.close('all')  # get ready\n\n    try:\n        globals()[runFunction](abf)  # run that function\n    except Exception as e:\n        abf.log.error(\"EXCEPTION DURING PROTOCOL FUNCTION\")\n        abf.log.error(e)\n        return \"ERROR\"\n\n    plt.close('all')  # clean up\n    return \"SUCCESS\"", "Function Name 1": "analyze", "Function Name 2": "autobuild", "Function Name 3": "runprotocol", "Function Name 4": "execute_protocol"}
{"idx": 21, "code": "def XXXXX(x, rho, penalty, axis=0, newshape=None):\n    orig_shape = x.shape\n\n    if newshape is not None:\n        x = x.reshape(newshape)\n\n    # Apply Laplacian smoothing (l2 norm on the parameters multiplied by\n    # the laplacian)\n    n = x.shape[axis]\n    lap_op = spdiags([(2 + rho / penalty) * np.ones(n),\n                      -1 * np.ones(n),\n                      -1 * np.ones(n)],\n                     [0, -1, 1], n, n, format='csc')\n    A = penalty * lap_op\n    b = rho * np.rollaxis(x, axis, 0)\n\n    return np.rollaxis(spsolve(A, b), axis, 0).reshape(orig_shape)", "Function Name 1": "smooth", "Function Name 2": "fit", "Function Name 3": "laplacian", "Function Name 4": "laplacian_solver"}
{"idx": 22, "code": "def XXXXX(n):\n    checksum = 10 - (sum([int(c) * i for c, i in zip(str(n)[::-1], it.cycle((7, 3, 1)))]) % 10)\n    if checksum == 10:\n        checksum = 0\n    return \"%s%s\" % (n, checksum)", "Function Name 1": "int2fin_reference", "Function Name 2": "calculate", "Function Name 3": "addchecksum", "Function Name 4": "calculatechecksum"}
{"idx": 23, "code": "def XXXXX(isocode, lang=\"eng\"):\n    if lang not in flask_nemo._data.AVAILABLE_TRANSLATIONS:\n        lang = \"eng\"\n\n    try:\n        return flask_nemo._data.ISOCODES[isocode][lang]\n    except KeyError:\n        return \"Unknown\"", "Function Name 1": "f_i18n_iso", "Function Name 2": "getiso", "Function Name 3": "getisocode", "Function Name 4": "eng"}
{"idx": 24, "code": "def XXXXX(self, timeout=5):\n    host = self._connect_args['host']\n    port = self._connect_args['port']\n\n    try:\n        sock = socket.create_connection((host, port), timeout=timeout)\n        sock.close()\n        return True\n    except socket.error:\n        pass\n\n    return False", "Function Name 1": "available", "Function Name 2": "tcpconnection", "Function Name 3": "isconnected", "Function Name 4": "check_connection"}
